{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-01T01:06:07.874730Z",
     "iopub.status.busy": "2024-11-01T01:06:07.874444Z",
     "iopub.status.idle": "2024-11-01T01:07:01.232011Z",
     "shell.execute_reply": "2024-11-01T01:07:01.230998Z",
     "shell.execute_reply.started": "2024-11-01T01:06:07.874696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tree_sitter import Language, Parser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Install required packages\n",
    "# !pip install tree-sitter\n",
    "# !pip install tree-sitter-java\n",
    "# !pip install tree-sitter-python\n",
    "# !pip install tree-sitter-cpp\n",
    "# !pip install tree-sitter-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:01.234232Z",
     "iopub.status.busy": "2024-11-01T01:07:01.233798Z",
     "iopub.status.idle": "2024-11-01T01:07:01.242273Z",
     "shell.execute_reply": "2024-11-01T01:07:01.241388Z",
     "shell.execute_reply.started": "2024-11-01T01:07:01.234196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Java, Python, and C++ languages\n",
    "import tree_sitter_java\n",
    "import tree_sitter_python\n",
    "import tree_sitter_cpp\n",
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "PYTHON_LANGUAGE = Language(tree_sitter_python.language())\n",
    "CPP_LANGUAGE = Language(tree_sitter_cpp.language())\n",
    "\n",
    "java_parser = Parser(JAVA_LANGUAGE)\n",
    "python_parser = Parser(PYTHON_LANGUAGE)\n",
    "cpp_parser = Parser(CPP_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:01.244255Z",
     "iopub.status.busy": "2024-11-01T01:07:01.243940Z",
     "iopub.status.idle": "2024-11-01T01:07:06.195853Z",
     "shell.execute_reply": "2024-11-01T01:07:06.194933Z",
     "shell.execute_reply.started": "2024-11-01T01:07:01.244223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CodeBERT model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.198237Z",
     "iopub.status.busy": "2024-11-01T01:07:06.197924Z",
     "iopub.status.idle": "2024-11-01T01:07:06.204635Z",
     "shell.execute_reply": "2024-11-01T01:07:06.203717Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.198204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.206215Z",
     "iopub.status.busy": "2024-11-01T01:07:06.205838Z",
     "iopub.status.idle": "2024-11-01T01:07:06.217450Z",
     "shell.execute_reply": "2024-11-01T01:07:06.216682Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.206168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tree_to_sequence(code, language):\n",
    "    if language == 'java':\n",
    "        parser = java_parser\n",
    "    elif language == 'python':\n",
    "        parser = python_parser\n",
    "    elif language == 'cpp':\n",
    "        parser = cpp_parser\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "\n",
    "    def traverse(node, depth=0):\n",
    "        if node.type != 'string' and node.type != 'comment':\n",
    "            yield f\"{node.type}_{depth}\"\n",
    "            for child in node.children:\n",
    "                yield from traverse(child, depth + 1)\n",
    "\n",
    "    return ' '.join(traverse(tree.root_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.219900Z",
     "iopub.status.busy": "2024-11-01T01:07:06.218918Z",
     "iopub.status.idle": "2024-11-01T01:07:06.230115Z",
     "shell.execute_reply": "2024-11-01T01:07:06.229163Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.219863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_code(code, language):\n",
    "    # Remove comments\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    elif language == 'python':\n",
    "        code = re.sub(r'#.*?\\n|\\'\\'\\'.*?\\'\\'\\'|\"\"\".*?\"\"\"', '', code, flags=re.DOTALL)\n",
    "    elif language == 'cpp':\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove string literals\n",
    "    code = re.sub(r'\".*?\"', '\"\"', code)\n",
    "\n",
    "    # Remove import statements\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'import\\s+[\\w.]+;', '', code)\n",
    "    elif language == 'python':\n",
    "        code = re.sub(r'import\\s+[\\w.]+|from\\s+[\\w.]+\\s+import\\s+[\\w.]+', '', code)\n",
    "    elif language == 'cpp':\n",
    "        code = re.sub(r'#include\\s+<[\\w.]+>|#include\\s+\"[\\w.]+\"', '', code)\n",
    "\n",
    "    # Remove package declarations (Java only)\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'package\\s+[\\w.]+;', '', code)\n",
    "\n",
    "    # Remove whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.231579Z",
     "iopub.status.busy": "2024-11-01T01:07:06.231278Z",
     "iopub.status.idle": "2024-11-01T01:07:06.238316Z",
     "shell.execute_reply": "2024-11-01T01:07:06.237480Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.231546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.239825Z",
     "iopub.status.busy": "2024-11-01T01:07:06.239491Z",
     "iopub.status.idle": "2024-11-01T01:07:06.245890Z",
     "shell.execute_reply": "2024-11-01T01:07:06.244994Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.239782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalized_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.247253Z",
     "iopub.status.busy": "2024-11-01T01:07:06.246958Z",
     "iopub.status.idle": "2024-11-01T01:07:06.254348Z",
     "shell.execute_reply": "2024-11-01T01:07:06.253521Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.247221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_file_language(filename):\n",
    "    extension = os.path.splitext(filename)[1].lower()\n",
    "    if extension in ['.java']:\n",
    "        return 'java'\n",
    "    elif extension in ['.py', '.pyw']:\n",
    "        return 'python'\n",
    "    elif extension in ['.cpp', '.cxx', '.cc', '.c++', '.hpp', '.hxx', '.hh', '.h++', '.h']:\n",
    "        return 'cpp'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.257815Z",
     "iopub.status.busy": "2024-11-01T01:07:06.257500Z",
     "iopub.status.idle": "2024-11-01T01:07:06.266099Z",
     "shell.execute_reply": "2024-11-01T01:07:06.265220Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.257782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_codebert_embedding(code, tokenizer_type='default'):\n",
    "    try:\n",
    "        if tokenizer_type == 'default':\n",
    "            inputs = tokenizer(code, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        elif tokenizer_type == 'word':\n",
    "            tokens = code.split()\n",
    "            inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        elif tokenizer_type == 'character':\n",
    "            tokens = list(code)\n",
    "            inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported tokenizer type: {tokenizer_type}\")\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating CodeBERT embedding: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:07:06.267450Z",
     "iopub.status.busy": "2024-11-01T01:07:06.267173Z",
     "iopub.status.idle": "2024-11-01T01:07:06.276356Z",
     "shell.execute_reply": "2024-11-01T01:07:06.275437Z",
     "shell.execute_reply.started": "2024-11-01T01:07:06.267420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_files(directory, tokenizer_type='default', use_tree_sitter=False):\n",
    "    submissions = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                language = get_file_language(file)\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        code = f.read()\n",
    "                        preprocessed_code = preprocess_code(code, language)\n",
    "                        if use_tree_sitter:\n",
    "                            tree_sequence = tree_to_sequence(preprocessed_code, language)\n",
    "                            codebert_embedding = get_codebert_embedding(tree_sequence, tokenizer_type)\n",
    "                            tokens = set(tree_sequence.split())\n",
    "                        else:\n",
    "                            codebert_embedding = get_codebert_embedding(preprocessed_code, tokenizer_type)\n",
    "                            tokens = set(preprocessed_code.split())\n",
    "                        submission = {\n",
    "                            'sequence': tree_sequence if use_tree_sitter else preprocessed_code,\n",
    "                            'language': language,\n",
    "                            'embedding': codebert_embedding,\n",
    "                            'tokens': tokens\n",
    "                        }\n",
    "                        submissions[file] = submission\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Error reading {file_path}. Skipping.\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping file {file}: {str(e)}\")\n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:16:14.329133Z",
     "iopub.status.busy": "2024-11-01T01:16:14.328714Z",
     "iopub.status.idle": "2024-11-01T01:16:14.335711Z",
     "shell.execute_reply": "2024-11-01T01:16:14.334635Z",
     "shell.execute_reply.started": "2024-11-01T01:16:14.329095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_all_combinations(directory):\n",
    "    tokenizer_options = ['default', 'word', 'character']\n",
    "    tree_sitter_options = [False, True]\n",
    "    all_results = {}\n",
    "\n",
    "    for tokenizer_type in tokenizer_options:\n",
    "        for use_tree_sitter in tree_sitter_options:\n",
    "            print(f\"Running with tokenizer: {tokenizer_type}, Tree-Sitter: {'Yes' if use_tree_sitter else 'No'}\")\n",
    "            plagiarism_results = check_plagiarism(directory, tokenizer_type=tokenizer_type, use_tree_sitter=use_tree_sitter)\n",
    "\n",
    "            key = f\"{tokenizer_type}_{'tree_sitter' if use_tree_sitter else 'no_tree_sitter'}\"\n",
    "            all_results[key] = plagiarism_results\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:16:16.488510Z",
     "iopub.status.busy": "2024-11-01T01:16:16.487893Z",
     "iopub.status.idle": "2024-11-01T01:16:16.498888Z",
     "shell.execute_reply": "2024-11-01T01:16:16.498061Z",
     "shell.execute_reply.started": "2024-11-01T01:16:16.488457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_similarities(submissions):\n",
    "    filenames = list(submissions.keys())\n",
    "    n = len(filenames)\n",
    "    semantic_similarities = np.zeros((n, n))\n",
    "    token_similarities = np.zeros((n, n))\n",
    "    structural_similarities = np.zeros((n, n))\n",
    "\n",
    "    # Prepare TF-IDF vectorizer for structural similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([sub['sequence'] for sub in submissions.values()])\n",
    "\n",
    "    embeddings = np.array([sub['embedding'] for sub in submissions.values()])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            # Semantic similarity (formerly CodeBERT)\n",
    "            semantic_sim = normalized_similarity(embeddings[i], embeddings[j])\n",
    "            semantic_similarities[i][j] = semantic_similarities[j][i] = semantic_sim * 100\n",
    "\n",
    "            # Token similarity (formerly Jaccard)\n",
    "            token_sim = jaccard_similarity(submissions[filenames[i]]['tokens'], submissions[filenames[j]]['tokens'])\n",
    "            token_similarities[i][j] = token_similarities[j][i] = token_sim * 100\n",
    "\n",
    "            # Structural similarity (formerly TF-IDF)\n",
    "            structural_sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n",
    "            structural_similarities[i][j] = structural_similarities[j][i] = structural_sim * 100\n",
    "\n",
    "    return semantic_similarities, token_similarities, structural_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:16:19.032033Z",
     "iopub.status.busy": "2024-11-01T01:16:19.031170Z",
     "iopub.status.idle": "2024-11-01T01:16:19.041592Z",
     "shell.execute_reply": "2024-11-01T01:16:19.040570Z",
     "shell.execute_reply.started": "2024-11-01T01:16:19.031992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_plagiarism(directory, threshold=75, tokenizer_type='default', use_tree_sitter=False):\n",
    "    submissions = process_files(directory, tokenizer_type, use_tree_sitter)\n",
    "    semantic_similarities, token_similarities, structural_similarities = compute_similarities(submissions)\n",
    "\n",
    "    filenames = list(submissions.keys())\n",
    "    n = len(filenames)\n",
    "\n",
    "    results = []\n",
    "    total_similarity = 0\n",
    "    comparison_count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        file_result = {\"file\": filenames[i], \"comparisons\": {}}\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                semantic_sim = semantic_similarities[i][j]\n",
    "                token_sim = token_similarities[i][j]\n",
    "                structural_sim = structural_similarities[i][j]\n",
    "\n",
    "                # Calculate weighted combined similarity\n",
    "                combined_sim = (\n",
    "                    token_sim * 0.45 +          # Token similarity weight\n",
    "                    structural_sim * 0.45 +      # Structural similarity weight\n",
    "                    semantic_sim * 0.05          # Semantic similarity weight\n",
    "                )\n",
    "\n",
    "                file_result[\"comparisons\"][filenames[j]] = {\n",
    "                    \"token_similarity\": token_sim,\n",
    "                    \"structural_similarity\": structural_sim,\n",
    "                    \"semantic_similarity\": semantic_sim,\n",
    "                    \"combined_similarity\": combined_sim,\n",
    "                    \"potential_plagiarism\": combined_sim > threshold\n",
    "                }\n",
    "\n",
    "                total_similarity += combined_sim\n",
    "                comparison_count += 1\n",
    "\n",
    "        results.append(file_result)\n",
    "\n",
    "    average_similarity = total_similarity / comparison_count if comparison_count > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"weights\": {\n",
    "            \"token_similarity\": 0.45,\n",
    "            \"structural_similarity\": 0.45,\n",
    "            \"semantic_similarity\": 0.05\n",
    "        },\n",
    "        \"average_similarity\": average_similarity,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case-01', 'case-02', 'case-03', 'case-04', 'case-05', 'case-06', 'case-07']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'C:\\CODES\\codeComparisonSimpleUI\\IR-Plag-Dataset\\IR-Plag-Dataset'  # replace with the actual path\n",
    "files = os.listdir(dataset_dir)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T01:16:22.596243Z",
     "iopub.status.busy": "2024-11-01T01:16:22.595559Z",
     "iopub.status.idle": "2024-11-01T01:16:44.144716Z",
     "shell.execute_reply": "2024-11-01T01:16:44.143734Z",
     "shell.execute_reply.started": "2024-11-01T01:16:22.596201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an analysis type:\n",
      "1. Single configuration\n",
      "2. All combinations\n",
      "Choose a tokenizer:\n",
      "1. Default\n",
      "2. Word\n",
      "3. Character\n",
      "Results have been saved to 'plagiarism_results_default_no_tree_sitter.json'\n",
      "Average similarity score: 25.58\n"
     ]
    }
   ],
   "source": [
    "# User input for analysis type\n",
    "print(\"Choose an analysis type:\")\n",
    "print(\"1. Single configuration\")\n",
    "print(\"2. All combinations\")\n",
    "analysis_choice = input(\"Enter your choice (1/2): \")\n",
    "\n",
    "if analysis_choice == '1':\n",
    "    # Single configuration\n",
    "    print(\"Choose a tokenizer:\")\n",
    "    print(\"1. Default\")\n",
    "    print(\"2. Word\")\n",
    "    print(\"3. Character\")\n",
    "    tokenizer_choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "    if tokenizer_choice == '1':\n",
    "        tokenizer_type = 'default'\n",
    "    elif tokenizer_choice == '2':\n",
    "        tokenizer_type = 'word'\n",
    "    elif tokenizer_choice == '3':\n",
    "        tokenizer_type = 'character'\n",
    "    else:\n",
    "        print(\"Invalid choice. Using default tokenizer.\")\n",
    "        tokenizer_type = 'default'\n",
    "\n",
    "    use_tree_sitter = input(\"Use Tree-Sitter? (y/n): \").lower() == 'y'\n",
    "\n",
    "    # Example usage\n",
    "    directory = 'C:\\CODES\\codeComparisonSimpleUI\\IR-Plag-Dataset\\IR-Plag-Dataset'\n",
    "    plagiarism_results = check_plagiarism(directory, tokenizer_type=tokenizer_type, use_tree_sitter=use_tree_sitter)\n",
    "\n",
    "    # Save to JSON file using the custom encoder\n",
    "    filename = f'plagiarism_results_{tokenizer_type}_{\"tree_sitter\" if use_tree_sitter else \"no_tree_sitter\"}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(plagiarism_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"Results have been saved to '{filename}'\")\n",
    "    print(f\"Average similarity score: {plagiarism_results['average_similarity']:.2f}\")\n",
    "\n",
    "elif analysis_choice == '2':\n",
    "    # All combinations\n",
    "    directory = 'C:\\CODES\\codeComparisonSimpleUI\\IR-Plag-Dataset\\IR-Plag-Dataset'\n",
    "    all_results = run_all_combinations(directory)\n",
    "\n",
    "    # Save all results to a single JSON file\n",
    "    filename = 'plagiarism_results_all_combinations.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"Results for all combinations have been saved to '{filename}'\")\n",
    "    \n",
    "    # Print average similarity scores for each combination\n",
    "    print(\"\\nAverage similarity scores:\")\n",
    "    for key, results in all_results.items():\n",
    "        print(f\"{key}: {results['average_similarity']:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5802482,
     "sourceId": 9528434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
