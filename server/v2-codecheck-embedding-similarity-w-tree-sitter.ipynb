{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9528434,"sourceType":"datasetVersion","datasetId":5802482}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install tree-sitter\n!pip install tree-sitter-java\n!pip install tree-sitter-python\n!pip install tree-sitter-cpp\n\nimport os\nimport numpy as np\nimport json\nfrom tqdm import tqdm\nfrom tree_sitter import Language, Parser\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import RobertaTokenizer, RobertaModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T08:36:15.827991Z","iopub.execute_input":"2024-10-18T08:36:15.828653Z","iopub.status.idle":"2024-10-18T08:37:08.950723Z","shell.execute_reply.started":"2024-10-18T08:36:15.828601Z","shell.execute_reply":"2024-10-18T08:37:08.949916Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting tree-sitter\n  Downloading tree_sitter-0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\nDownloading tree_sitter-0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (560 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m560.8/560.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tree-sitter\nSuccessfully installed tree-sitter-0.23.1\nCollecting tree-sitter-java\n  Downloading tree_sitter_java-0.23.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nDownloading tree_sitter_java-0.23.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tree-sitter-java\nSuccessfully installed tree-sitter-java-0.23.2\nCollecting tree-sitter-python\n  Downloading tree_sitter_python-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nDownloading tree_sitter_python-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tree-sitter-python\nSuccessfully installed tree-sitter-python-0.23.2\nCollecting tree-sitter-cpp\n  Downloading tree_sitter_cpp-0.23.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nDownloading tree_sitter_cpp-0.23.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tree-sitter-cpp\nSuccessfully installed tree-sitter-cpp-0.23.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load Java, Python, and C++ languages\nimport tree_sitter_java\nimport tree_sitter_python\nimport tree_sitter_cpp\n\nJAVA_LANGUAGE = Language(tree_sitter_java.language())\nPYTHON_LANGUAGE = Language(tree_sitter_python.language())\nCPP_LANGUAGE = Language(tree_sitter_cpp.language())\n\njava_parser = Parser(JAVA_LANGUAGE)\npython_parser = Parser(PYTHON_LANGUAGE)\ncpp_parser = Parser(CPP_LANGUAGE)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:08.952456Z","iopub.execute_input":"2024-10-18T08:37:08.952872Z","iopub.status.idle":"2024-10-18T08:37:08.960918Z","shell.execute_reply.started":"2024-10-18T08:37:08.952839Z","shell.execute_reply":"2024-10-18T08:37:08.960141Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load CodeBERT model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\nmodel = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:08.961982Z","iopub.execute_input":"2024-10-18T08:37:08.962329Z","iopub.status.idle":"2024-10-18T08:37:13.829637Z","shell.execute_reply.started":"2024-10-18T08:37:08.962280Z","shell.execute_reply":"2024-10-18T08:37:13.828708Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c54e2127844c77aada9af7a7d53b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5e82b4d32b47d2954eab5a62875e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0908cbab460f41258549e79ecadb609f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0da6482a6564cae8a266c8a5a55cbeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59df894f9ccd482081d080b963e33c34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654fe467f0684fb2b2a03790e0392a26"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, np.bool_):\n            return bool(obj)\n        return json.JSONEncoder.default(self, obj)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.830879Z","iopub.execute_input":"2024-10-18T08:37:13.831214Z","iopub.status.idle":"2024-10-18T08:37:13.837235Z","shell.execute_reply.started":"2024-10-18T08:37:13.831179Z","shell.execute_reply":"2024-10-18T08:37:13.836123Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def tree_to_sequence(code, language):\n    if language == 'java':\n        parser = java_parser\n    elif language == 'python':\n        parser = python_parser\n    elif language == 'cpp':\n        parser = cpp_parser\n    else:\n        raise ValueError(\"Unsupported language\")\n\n    tree = parser.parse(bytes(code, \"utf8\"))\n\n    def traverse(node, depth=0):\n        if node.type != 'string' and node.type != 'comment':\n            yield f\"{node.type}_{depth}\"\n            for child in node.children:\n                yield from traverse(child, depth + 1)\n\n    return ' '.join(traverse(tree.root_node))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.839915Z","iopub.execute_input":"2024-10-18T08:37:13.840209Z","iopub.status.idle":"2024-10-18T08:37:13.848424Z","shell.execute_reply.started":"2024-10-18T08:37:13.840177Z","shell.execute_reply":"2024-10-18T08:37:13.847602Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def preprocess_code(code, language):\n    # Remove comments\n    if language == 'java':\n        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code, flags=re.DOTALL)\n    elif language == 'python':\n        code = re.sub(r'#.*?\\n|\\'\\'\\'.*?\\'\\'\\'|\"\"\".*?\"\"\"', '', code, flags=re.DOTALL)\n\n    # Remove string literals\n    code = re.sub(r'\".*?\"', '\"\"', code)\n\n    # Remove import statements\n    if language == 'java':\n        code = re.sub(r'import\\s+[\\w.]+;', '', code)\n    elif language == 'python':\n        code = re.sub(r'import\\s+[\\w.]+|from\\s+[\\w.]+\\s+import\\s+[\\w.]+', '', code)\n\n    # Remove package declarations (Java only)\n    if language == 'java':\n        code = re.sub(r'package\\s+[\\w.]+;', '', code)\n\n    # Remove whitespace\n    code = re.sub(r'\\s+', ' ', code).strip()\n    return code","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.849416Z","iopub.execute_input":"2024-10-18T08:37:13.849676Z","iopub.status.idle":"2024-10-18T08:37:13.860568Z","shell.execute_reply.started":"2024-10-18T08:37:13.849647Z","shell.execute_reply":"2024-10-18T08:37:13.859659Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def jaccard_similarity(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union != 0 else 0","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.861709Z","iopub.execute_input":"2024-10-18T08:37:13.862055Z","iopub.status.idle":"2024-10-18T08:37:13.869004Z","shell.execute_reply.started":"2024-10-18T08:37:13.862013Z","shell.execute_reply":"2024-10-18T08:37:13.868304Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def normalized_similarity(vec1, vec2):\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.870068Z","iopub.execute_input":"2024-10-18T08:37:13.870376Z","iopub.status.idle":"2024-10-18T08:37:13.877609Z","shell.execute_reply.started":"2024-10-18T08:37:13.870340Z","shell.execute_reply":"2024-10-18T08:37:13.876700Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def get_file_language(filename):\n    extension = os.path.splitext(filename)[1].lower()\n    if extension in ['.java']:\n        return 'java'\n    elif extension in ['.py', '.pyw']:\n        return 'python'\n    elif extension in ['.cpp', '.cxx', '.cc', '.c++', '.hpp', '.hxx', '.hh', '.h++', '.h']:\n        return 'cpp'\n    else:\n        raise ValueError(f\"Unsupported file type: {filename}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.878776Z","iopub.execute_input":"2024-10-18T08:37:13.879147Z","iopub.status.idle":"2024-10-18T08:37:13.887140Z","shell.execute_reply.started":"2024-10-18T08:37:13.879107Z","shell.execute_reply":"2024-10-18T08:37:13.885809Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_codebert_embedding(code):\n    try:\n        inputs = tokenizer(code, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n    except Exception as e:\n        print(f\"Error generating CodeBERT embedding: {str(e)}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.888268Z","iopub.execute_input":"2024-10-18T08:37:13.888583Z","iopub.status.idle":"2024-10-18T08:37:13.897861Z","shell.execute_reply.started":"2024-10-18T08:37:13.888539Z","shell.execute_reply":"2024-10-18T08:37:13.896995Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def process_files(directory):\n    submissions = {}\n    for root, _, files in os.walk(directory):\n        for file in files:\n            try:\n                language = get_file_language(file)\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    try:\n                        code = f.read()\n                        preprocessed_code = preprocess_code(code, language)\n                        tree_sequence = tree_to_sequence(preprocessed_code, language)\n                        codebert_embedding = get_codebert_embedding(tree_sequence)\n                        submission = {\n                            'sequence': tree_sequence,\n                            'language': language,\n                            'embedding': codebert_embedding,\n                            'tokens': set(tree_sequence.split())\n                        }\n                        submissions[file] = submission\n                    except UnicodeDecodeError:\n                        print(f\"Error reading {file_path}. Skipping.\")\n            except ValueError as e:\n                print(f\"Skipping file {file}: {str(e)}\")\n    return submissions\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.899082Z","iopub.execute_input":"2024-10-18T08:37:13.899661Z","iopub.status.idle":"2024-10-18T08:37:13.907659Z","shell.execute_reply.started":"2024-10-18T08:37:13.899628Z","shell.execute_reply":"2024-10-18T08:37:13.906744Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def compute_similarities(submissions):\n    filenames = list(submissions.keys())\n    n = len(filenames)\n    codebert_similarities = np.zeros((n, n))\n    jaccard_similarities = np.zeros((n, n))\n    tfidf_similarities = np.zeros((n, n))\n\n    # Prepare TF-IDF vectorizer\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([sub['sequence'] for sub in submissions.values()])\n\n    embeddings = np.array([sub['embedding'] for sub in submissions.values()])\n\n    for i in range(n):\n        for j in range(i+1, n):\n            # CodeBERT similarity\n            codebert_sim = normalized_similarity(embeddings[i], embeddings[j])\n            codebert_similarities[i][j] = codebert_similarities[j][i] = codebert_sim * 100\n\n            # Jaccard similarity\n            jaccard_sim = jaccard_similarity(submissions[filenames[i]]['tokens'], submissions[filenames[j]]['tokens'])\n            jaccard_similarities[i][j] = jaccard_similarities[j][i] = jaccard_sim * 100\n\n            # TF-IDF similarity\n            tfidf_sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n            tfidf_similarities[i][j] = tfidf_similarities[j][i] = tfidf_sim * 100\n\n    return codebert_similarities, jaccard_similarities, tfidf_similarities","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.908728Z","iopub.execute_input":"2024-10-18T08:37:13.909068Z","iopub.status.idle":"2024-10-18T08:37:13.920312Z","shell.execute_reply.started":"2024-10-18T08:37:13.909030Z","shell.execute_reply":"2024-10-18T08:37:13.919578Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def check_plagiarism(directory, threshold=80):\n    submissions = process_files(directory)\n    codebert_similarities, jaccard_similarities, tfidf_similarities = compute_similarities(submissions)\n\n    filenames = list(submissions.keys())\n    n = len(filenames)\n\n    results = []\n    for i in range(n):\n        file_result = {\"file\": filenames[i], \"comparisons\": {}}\n        for j in range(n):\n            if i != j:\n                codebert_sim = codebert_similarities[i][j]\n                jaccard_sim = jaccard_similarities[i][j]\n                tfidf_sim = tfidf_similarities[i][j]\n\n                # Calculate combined similarity (you can adjust the weights as needed)\n                combined_sim = (codebert_sim + jaccard_sim + tfidf_sim) / 3\n\n                file_result[\"comparisons\"] = {\n                    \"filename\": [filenames[j]] ,\n                    \"combined_similarity\": combined_sim,\n                    \"potential_plagiarism\": combined_sim > threshold\n                }\n        results.append(file_result)\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.921447Z","iopub.execute_input":"2024-10-18T08:37:13.921740Z","iopub.status.idle":"2024-10-18T08:37:13.931740Z","shell.execute_reply.started":"2024-10-18T08:37:13.921708Z","shell.execute_reply":"2024-10-18T08:37:13.931058Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Example usage\ndirectory = '/kaggle/input/ir-plag-dataset'\nplagiarism_results = check_plagiarism(directory)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:13.936810Z","iopub.execute_input":"2024-10-18T08:37:13.937239Z","iopub.status.idle":"2024-10-18T08:37:14.650409Z","shell.execute_reply.started":"2024-10-18T08:37:13.937209Z","shell.execute_reply":"2024-10-18T08:37:14.646296Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/ir-plag-dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m plagiarism_results \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_plagiarism\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mcheck_plagiarism\u001b[0;34m(directory, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_plagiarism\u001b[39m(directory, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m):\n\u001b[1;32m      2\u001b[0m     submissions \u001b[38;5;241m=\u001b[39m process_files(directory)\n\u001b[0;32m----> 3\u001b[0m     codebert_similarities, jaccard_similarities, tfidf_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmissions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(submissions\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      6\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filenames)\n","Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mcompute_similarities\u001b[0;34m(submissions)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare TF-IDF vectorizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m---> 10\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msub\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmissions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m submissions\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"],"ename":"ValueError","evalue":"empty vocabulary; perhaps the documents only contain stop words","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# Save to JSON file using the custom encoder\nwith open('plagiarism_results.json', 'w') as f:\n    json.dump(plagiarism_results, f, indent=2, cls=NumpyEncoder)\n\nprint(\"Results have been saved to 'plagiarism_results.json'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T08:37:14.651039Z","iopub.status.idle":"2024-10-18T08:37:14.651408Z","shell.execute_reply.started":"2024-10-18T08:37:14.651207Z","shell.execute_reply":"2024-10-18T08:37:14.651224Z"},"trusted":true},"outputs":[],"execution_count":null}]}