{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-18T08:36:15.828653Z",
     "iopub.status.busy": "2024-10-18T08:36:15.827991Z",
     "iopub.status.idle": "2024-10-18T08:37:08.950723Z",
     "shell.execute_reply": "2024-10-18T08:37:08.949916Z",
     "shell.execute_reply.started": "2024-10-18T08:36:15.828601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter-java in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter-python in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.23.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tree-sitter-cpp in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.23.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter-python in c:\\users\\gabri\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\gabri\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install tree-sitter\n",
    "!pip install tree-sitter-java\n",
    "!pip install tree-sitter-python\n",
    "!pip install tree-sitter-cpp\n",
    "!pip install tree-sitter-python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tree_sitter import Language, Parser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:08.952872Z",
     "iopub.status.busy": "2024-10-18T08:37:08.952456Z",
     "iopub.status.idle": "2024-10-18T08:37:08.960918Z",
     "shell.execute_reply": "2024-10-18T08:37:08.960141Z",
     "shell.execute_reply.started": "2024-10-18T08:37:08.952839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Java, Python, and C++ languages\n",
    "import tree_sitter_java\n",
    "import tree_sitter_python\n",
    "import tree_sitter_cpp\n",
    "\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "PYTHON_LANGUAGE = Language(tree_sitter_python.language())\n",
    "CPP_LANGUAGE = Language(tree_sitter_cpp.language())\n",
    "\n",
    "java_parser = Parser(JAVA_LANGUAGE)\n",
    "python_parser = Parser(PYTHON_LANGUAGE)\n",
    "cpp_parser = Parser(CPP_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:08.962329Z",
     "iopub.status.busy": "2024-10-18T08:37:08.961982Z",
     "iopub.status.idle": "2024-10-18T08:37:13.829637Z",
     "shell.execute_reply": "2024-10-18T08:37:13.828708Z",
     "shell.execute_reply.started": "2024-10-18T08:37:08.962280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gabri\\.cache\\huggingface\\hub\\models--microsoft--codebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CodeBERT model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.831214Z",
     "iopub.status.busy": "2024-10-18T08:37:13.830879Z",
     "iopub.status.idle": "2024-10-18T08:37:13.837235Z",
     "shell.execute_reply": "2024-10-18T08:37:13.836123Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.831179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.840209Z",
     "iopub.status.busy": "2024-10-18T08:37:13.839915Z",
     "iopub.status.idle": "2024-10-18T08:37:13.848424Z",
     "shell.execute_reply": "2024-10-18T08:37:13.847602Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.840177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tree_to_sequence(code, language):\n",
    "    if language == 'java':\n",
    "        parser = java_parser\n",
    "    elif language == 'python':\n",
    "        parser = python_parser\n",
    "    elif language == 'cpp':\n",
    "        parser = cpp_parser\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "\n",
    "    def traverse(node, depth=0):\n",
    "        if node.type != 'string' and node.type != 'comment':\n",
    "            yield f\"{node.type}_{depth}\"\n",
    "            for child in node.children:\n",
    "                yield from traverse(child, depth + 1)\n",
    "\n",
    "    return ' '.join(traverse(tree.root_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.849676Z",
     "iopub.status.busy": "2024-10-18T08:37:13.849416Z",
     "iopub.status.idle": "2024-10-18T08:37:13.860568Z",
     "shell.execute_reply": "2024-10-18T08:37:13.859659Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.849647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_code(code, language):\n",
    "    # Remove comments\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    elif language == 'python':\n",
    "        code = re.sub(r'#.*?\\n|\\'\\'\\'.*?\\'\\'\\'|\"\"\".*?\"\"\"', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove string literals\n",
    "    code = re.sub(r'\".*?\"', '\"\"', code)\n",
    "\n",
    "    # Remove import statements\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'import\\s+[\\w.]+;', '', code)\n",
    "    elif language == 'python':\n",
    "        code = re.sub(r'import\\s+[\\w.]+|from\\s+[\\w.]+\\s+import\\s+[\\w.]+', '', code)\n",
    "\n",
    "    # Remove package declarations (Java only)\n",
    "    if language == 'java':\n",
    "        code = re.sub(r'package\\s+[\\w.]+;', '', code)\n",
    "\n",
    "    # Remove whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.862055Z",
     "iopub.status.busy": "2024-10-18T08:37:13.861709Z",
     "iopub.status.idle": "2024-10-18T08:37:13.869004Z",
     "shell.execute_reply": "2024-10-18T08:37:13.868304Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.862013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.870376Z",
     "iopub.status.busy": "2024-10-18T08:37:13.870068Z",
     "iopub.status.idle": "2024-10-18T08:37:13.877609Z",
     "shell.execute_reply": "2024-10-18T08:37:13.876700Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.870340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalized_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.879147Z",
     "iopub.status.busy": "2024-10-18T08:37:13.878776Z",
     "iopub.status.idle": "2024-10-18T08:37:13.887140Z",
     "shell.execute_reply": "2024-10-18T08:37:13.885809Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.879107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_file_language(filename):\n",
    "    extension = os.path.splitext(filename)[1].lower()\n",
    "    if extension in ['.java']:\n",
    "        return 'java'\n",
    "    elif extension in ['.py', '.pyw']:\n",
    "        return 'python'\n",
    "    elif extension in ['.cpp', '.cxx', '.cc', '.c++', '.hpp', '.hxx', '.hh', '.h++', '.h']:\n",
    "        return 'cpp'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.888583Z",
     "iopub.status.busy": "2024-10-18T08:37:13.888268Z",
     "iopub.status.idle": "2024-10-18T08:37:13.897861Z",
     "shell.execute_reply": "2024-10-18T08:37:13.896995Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.888539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_codebert_embedding(code):\n",
    "    try:\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating CodeBERT embedding: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.899661Z",
     "iopub.status.busy": "2024-10-18T08:37:13.899082Z",
     "iopub.status.idle": "2024-10-18T08:37:13.907659Z",
     "shell.execute_reply": "2024-10-18T08:37:13.906744Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.899628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_files(directory):\n",
    "    submissions = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                language = get_file_language(file)\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        code = f.read()\n",
    "                        preprocessed_code = preprocess_code(code, language)\n",
    "                        tree_sequence = tree_to_sequence(preprocessed_code, language)\n",
    "                        codebert_embedding = get_codebert_embedding(tree_sequence)\n",
    "                        submission = {\n",
    "                            'sequence': tree_sequence,\n",
    "                            'language': language,\n",
    "                            'embedding': codebert_embedding,\n",
    "                            'tokens': set(tree_sequence.split())\n",
    "                        }\n",
    "                        submissions[file] = submission\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Error reading {file_path}. Skipping.\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping file {file}: {str(e)}\")\n",
    "    return submissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_combinations(directory):\n",
    "    tokenizer_options = ['default', 'word', 'character']\n",
    "    tree_sitter_options = [False, True]\n",
    "    all_results = {}\n",
    "\n",
    "    for tokenizer_type in tokenizer_options:\n",
    "        for use_tree_sitter in tree_sitter_options:\n",
    "            print(f\"Running with tokenizer: {tokenizer_type}, Tree-Sitter: {'Yes' if use_tree_sitter else 'No'}\")\n",
    "            plagiarism_results = check_plagiarism(directory, tokenizer_type=tokenizer_type, use_tree_sitter=use_tree_sitter)\n",
    "\n",
    "            key = f\"{tokenizer_type}_{'tree_sitter' if use_tree_sitter else 'no_tree_sitter'}\"\n",
    "            all_results[key] = plagiarism_results\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.909068Z",
     "iopub.status.busy": "2024-10-18T08:37:13.908728Z",
     "iopub.status.idle": "2024-10-18T08:37:13.920312Z",
     "shell.execute_reply": "2024-10-18T08:37:13.919578Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.909030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_similarities(submissions):\n",
    "    filenames = list(submissions.keys())\n",
    "    n = len(filenames)\n",
    "    semantic_similarities = np.zeros((n, n))\n",
    "    token_similarities = np.zeros((n, n))\n",
    "    structural_similarities = np.zeros((n, n))\n",
    "\n",
    "    # Prepare TF-IDF vectorizer for structural similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([sub['sequence'] for sub in submissions.values()])\n",
    "\n",
    "    embeddings = np.array([sub['embedding'] for sub in submissions.values()])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            # Semantic similarity (formerly CodeBERT)\n",
    "            semantic_sim = normalized_similarity(embeddings[i], embeddings[j])\n",
    "            semantic_similarities[i][j] = semantic_similarities[j][i] = semantic_sim * 100\n",
    "\n",
    "            # Token similarity (formerly Jaccard)\n",
    "            token_sim = jaccard_similarity(submissions[filenames[i]]['tokens'], submissions[filenames[j]]['tokens'])\n",
    "            token_similarities[i][j] = token_similarities[j][i] = token_sim * 100\n",
    "\n",
    "            # Structural similarity (formerly TF-IDF)\n",
    "            structural_sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n",
    "            structural_similarities[i][j] = structural_similarities[j][i] = structural_sim * 100\n",
    "\n",
    "    return semantic_similarities, token_similarities, structural_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T08:37:13.921740Z",
     "iopub.status.busy": "2024-10-18T08:37:13.921447Z",
     "iopub.status.idle": "2024-10-18T08:37:13.931740Z",
     "shell.execute_reply": "2024-10-18T08:37:13.931058Z",
     "shell.execute_reply.started": "2024-10-18T08:37:13.921708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_plagiarism(directory, threshold=75, tokenizer_type='default', use_tree_sitter=False):\n",
    "    submissions = process_files(directory, tokenizer_type, use_tree_sitter)\n",
    "    semantic_similarities, token_similarities, structural_similarities = compute_similarities(submissions)\n",
    "\n",
    "    filenames = list(submissions.keys())\n",
    "    n = len(filenames)\n",
    "\n",
    "    results = []\n",
    "    total_similarity = 0\n",
    "    comparison_count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        file_result = {\"file\": filenames[i], \"comparisons\": {}}\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                semantic_sim = semantic_similarities[i][j]\n",
    "                token_sim = token_similarities[i][j]\n",
    "                structural_sim = structural_similarities[i][j]\n",
    "\n",
    "                # Calculate weighted combined similarity\n",
    "                combined_sim = (\n",
    "                    token_sim * 0.45 +          # Token similarity weight\n",
    "                    structural_sim * 0.45 +      # Structural similarity weight\n",
    "                    semantic_sim * 0.05          # Semantic similarity weight\n",
    "                )\n",
    "\n",
    "                file_result[\"comparisons\"][filenames[j]] = {\n",
    "                    \"token_similarity\": token_sim,\n",
    "                    \"structural_similarity\": structural_sim,\n",
    "                    \"semantic_similarity\": semantic_sim,\n",
    "                    \"combined_similarity\": combined_sim,\n",
    "                    \"potential_plagiarism\": combined_sim > threshold\n",
    "                }\n",
    "\n",
    "                total_similarity += combined_sim\n",
    "                comparison_count += 1\n",
    "\n",
    "        results.append(file_result)\n",
    "\n",
    "    average_similarity = total_similarity / comparison_count if comparison_count > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"weights\": {\n",
    "            \"token_similarity\": 0.45,\n",
    "            \"structural_similarity\": 0.45,\n",
    "            \"semantic_similarity\": 0.05\n",
    "        },\n",
    "        \"average_similarity\": average_similarity,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an analysis type:\n",
      "1. Single configuration\n",
      "2. All combinations\n",
      "Choose a tokenizer:\n",
      "1. Default\n",
      "2. Word\n",
      "3. Character\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "process_files() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     28\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIR-Plag-Dataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIR-Plag-Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 29\u001b[0m plagiarism_results \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_plagiarism\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tree_sitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tree_sitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Save to JSON file using the custom encoder\u001b[39;00m\n\u001b[0;32m     32\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplagiarism_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_sitter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_tree_sitter\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_tree_sitter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mcheck_plagiarism\u001b[1;34m(directory, threshold, tokenizer_type, use_tree_sitter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_plagiarism\u001b[39m(directory, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m75\u001b[39m, tokenizer_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, use_tree_sitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     submissions \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tree_sitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     semantic_similarities, token_similarities, structural_similarities \u001b[38;5;241m=\u001b[39m compute_similarities(submissions)\n\u001b[0;32m      5\u001b[0m     filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(submissions\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mTypeError\u001b[0m: process_files() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# User input for analysis type\n",
    "print(\"Choose an analysis type:\")\n",
    "print(\"1. Single configuration\")\n",
    "print(\"2. All combinations\")\n",
    "analysis_choice = input(\"Enter your choice (1/2): \")\n",
    "\n",
    "if analysis_choice == '1':\n",
    "    # Single configuration\n",
    "    print(\"Choose a tokenizer:\")\n",
    "    print(\"1. Default\")\n",
    "    print(\"2. Word\")\n",
    "    print(\"3. Character\")\n",
    "    tokenizer_choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "    if tokenizer_choice == '1':\n",
    "        tokenizer_type = 'default'\n",
    "    elif tokenizer_choice == '2':\n",
    "        tokenizer_type = 'word'\n",
    "    elif tokenizer_choice == '3':\n",
    "        tokenizer_type = 'character'\n",
    "    else:\n",
    "        print(\"Invalid choice. Using default tokenizer.\")\n",
    "        tokenizer_type = 'default'\n",
    "\n",
    "    use_tree_sitter = input(\"Use Tree-Sitter? (y/n): \").lower() == 'y'\n",
    "\n",
    "    # Example usage\n",
    "    directory = 'IR-Plag-Dataset\\IR-Plag-Dataset'\n",
    "    plagiarism_results = check_plagiarism(directory, tokenizer_type=tokenizer_type, use_tree_sitter=use_tree_sitter)\n",
    "\n",
    "    # Save to JSON file using the custom encoder\n",
    "    filename = f'plagiarism_results_{tokenizer_type}_{\"tree_sitter\" if use_tree_sitter else \"no_tree_sitter\"}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(plagiarism_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"Results have been saved to '{filename}'\")\n",
    "    print(f\"Average similarity score: {plagiarism_results['average_similarity']:.2f}\")\n",
    "\n",
    "elif analysis_choice == '2':\n",
    "    # All combinations\n",
    "    directory = 'IR-Plag-Dataset\\IR-Plag-Dataset'\n",
    "    all_results = run_all_combinations(directory)\n",
    "\n",
    "    # Save all results to a single JSON file\n",
    "    filename = 'plagiarism_results_all_combinations.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"Results for all combinations have been saved to '{filename}'\")\n",
    "    \n",
    "    # Print average similarity scores for each combination\n",
    "    print(\"\\nAverage similarity scores:\")\n",
    "    for key, results in all_results.items():\n",
    "        print(f\"{key}: {results['average_similarity']:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-18T08:37:14.651039Z",
     "iopub.status.idle": "2024-10-18T08:37:14.651408Z",
     "shell.execute_reply": "2024-10-18T08:37:14.651224Z",
     "shell.execute_reply.started": "2024-10-18T08:37:14.651207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save to JSON file using the custom encoder\n",
    "with open('plagiarism_results.json', 'w') as f:\n",
    "    json.dump(plagiarism_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "print(\"Results have been saved to 'plagiarism_results.json'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5802482,
     "sourceId": 9528434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
